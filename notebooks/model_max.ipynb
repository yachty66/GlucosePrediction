{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath(os.getcwd())\n",
    "\n",
    "df_standards_warm = pd.read_csv(\"../data/standards_warm/Data_28_5__15_41_20_0_zu_2500.csv\")  \n",
    "df_warm_standards = pd.read_csv(\"../data/warm_standards_hamilton/Data_9_6__16_12_35_0_zu_2500.csv\")\n",
    "\n",
    "#todo ask what is valueRAW and what is value? my guess is that its the same data but just scaled somehow (see plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_warm_standards.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standards_warm.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = df_standards_warm[\"Time\"]\n",
    "values = df_standards_warm[\"Value\"]\n",
    "values_raw = df_standards_warm[\"ValueRAW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot time to values\n",
    "plt.plot(values,time)\n",
    "plt.yticks(range(0, len(time), 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot time to valuesRAW\n",
    "\n",
    "plt.plot(values_raw,time)\n",
    "plt.yticks(range(0, len(time), 100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data\n",
    "\n",
    "with open(\"../data/data_max/value_target.csv\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "with open(\"../data/data_max/value_target.csv\", \"a\") as f:\n",
    "    header = \"target\"\n",
    "    for i in range(1, 1001):\n",
    "        header = header + \",value_\" + str(i)\n",
    "    f.write(header + \"\\n\")\n",
    "\n",
    "#iter over every file from a folder\n",
    "for filename in os.listdir(\"../data/standards_warm/\"):\n",
    "    #open file\n",
    "    df_standards_warm = pd.read_csv(\"../data/standards_warm/\"+filename)  \n",
    "    #get the number which appears at the end between _...csv\n",
    "    number = filename.split(\"_\")[-1].split(\".\")[0]\n",
    "    #get the values from the column value\n",
    "    values = df_standards_warm[\"Value\"]\n",
    "    #create a new file with the name data_max/file\n",
    "    with open(\"../data/data_max/value_target.csv\", \"a\") as f:\n",
    "        #write the number which appears at the end between _...csv in a row of data/data_max/file\n",
    "        f.write(number)\n",
    "        #write a comma\n",
    "        f.write(\",\")\n",
    "        #write the values from the column value in a row of data/data_max/file\n",
    "        f.write(\",\".join(str(x) for x in values))\n",
    "        #write a new line\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "#iter over every file from a folder\n",
    "for filename in os.listdir(\"../data/warm_standards_hamilton\"):\n",
    "    #open file\n",
    "    df_standards_warm = pd.read_csv(\"../data/warm_standards_hamilton/\"+filename)  \n",
    "    #get the number which appears at the end between _...csv\n",
    "    number = filename.split(\"_\")[-1].split(\".\")[0]\n",
    "    #get the values from the column value\n",
    "    values = df_standards_warm[\"Value\"]\n",
    "    #create a new file with the name data_max/file\n",
    "    with open(\"../data/data_max/value_target.csv\", \"a\") as f:\n",
    "        #write the number which appears at the end between _...csv in a row of data/data_max/file\n",
    "        f.write(number)\n",
    "        #write a comma\n",
    "        f.write(\",\")\n",
    "        #write the values from the column value in a row of data/data_max/file\n",
    "        f.write(\",\".join(str(x) for x in values))\n",
    "        #write a new line\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_target = pd.read_csv(\"../data/data_max/value_target.csv\")\n",
    "\n",
    "test = df_value_target.tail(3)\n",
    "\n",
    "train = df_value_target.drop(df_value_target.tail(3).index)\n",
    "\n",
    "test_x = test.drop(\"target\", axis=1)\n",
    "\n",
    "test_y = test[\"target\"]\n",
    "\n",
    "train_x = train.drop(\"target\", axis=1)\n",
    "\n",
    "train_y = train[\"target\"]\n",
    "\n",
    "#later for creating onnx file necessary\n",
    "input_names = list(train_x)\n",
    "output_names = list(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.DataFrame(train_x)\n",
    "train_y = pd.DataFrame(train_y)\n",
    "test_x = pd.DataFrame(test_x)\n",
    "test_y = pd.DataFrame(test_y)\n",
    "\n",
    "scalerX = StandardScaler().fit(train_x)\n",
    "scalerY = StandardScaler().fit(train_y)\n",
    "train_x = scalerX.transform(train_x)\n",
    "train_y = scalerY.transform(train_y)\n",
    "test_x = scalerX.transform(test_x)\n",
    "test_y = scalerY.transform(test_y)\n",
    "\n",
    "model = LinearRegression().fit(train_x, train_y)\n",
    "\n",
    "predictions = model.predict(test_x)\n",
    "\n",
    "#print(predictions)\n",
    "print(test_y)\n",
    "#convert tensor to numpy\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "#calculate MAE \n",
    "mae = sum(abs(predictions - test_y))/len(predictions)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = pd.DataFrame(test_x)\n",
    "test_y = pd.DataFrame(test_y)\n",
    "train_x = pd.DataFrame(train_x)\n",
    "train_y = pd.DataFrame(train_y)\n",
    "\n",
    "#convert data to tensors\n",
    "test_x = torch.tensor(test_x.to_numpy()).float()\n",
    "test_y = torch.tensor(test_y.to_numpy()).float()\n",
    "train_x = torch.tensor(train_x.to_numpy()).float()\n",
    "train_y = torch.tensor(train_y.to_numpy()).float()\n",
    "\n",
    "#create a simple neural network with pytroch\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(1000, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1)\n",
    ")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(train_x)\n",
    "    loss = criterion(y_pred, train_y.float())\n",
    "    print(\"epoch: \", epoch, \"loss: \", loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#cacluate predictions\n",
    "pred_y = model(test_x.float())\n",
    "\n",
    "#calculate MAE\n",
    "mae = sum(abs(pred_y - test_y))/len(pred_y)\n",
    "#convert tensor to array \n",
    "mae = mae.detach().numpy()\n",
    "print(\"MAE: \" + str(mae[0]))\n",
    "\n",
    "#rescale data\n",
    "pred_y = scalerY.inverse_transform(pred_y.detach().numpy())\n",
    "test_y = scalerY.inverse_transform(test_y.detach().numpy())\n",
    "\n",
    "#print(pred_y)\n",
    "#print(test_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(train_x, train_y, test_x, test_y):\n",
    "    \"\"\"Scale data to standardization\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "        df: scaled data\n",
    "    \"\"\"\n",
    "    test_x = pd.DataFrame(test_x)\n",
    "    test_y = pd.DataFrame(test_y)\n",
    "    train_x = pd.DataFrame(train_x)\n",
    "    train_y = pd.DataFrame(train_y)\n",
    "    \n",
    "    scalerX = StandardScaler().fit(train_x)\n",
    "    scalerY = StandardScaler().fit(train_y)\n",
    "    train_x = scalerX.transform(train_x)\n",
    "    train_y = scalerY.transform(train_y)\n",
    "    test_x = scalerX.transform(test_x)\n",
    "    test_y = scalerY.transform(test_y)\n",
    "    \n",
    "    test_x = pd.DataFrame(test_x)\n",
    "    test_y = pd.DataFrame(test_y)\n",
    "    train_x = pd.DataFrame(train_x)\n",
    "    train_y = pd.DataFrame(train_y)\n",
    "\n",
    "    test_x = torch.tensor(test_x.to_numpy()).float()\n",
    "    test_y = torch.tensor(test_y.to_numpy()).float()\n",
    "    train_x = torch.tensor(train_x.to_numpy()).float()\n",
    "    train_y = torch.tensor(train_y.to_numpy()).float()\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "    \n",
    "def model_standard(train_x, train_y, test_x, test_y):\n",
    "    \"\"\"ML modell for glucose prediction\n",
    "\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "        int: mae of the model trained with data split based on cross validation\n",
    "    \"\"\"\n",
    "    train_x, train_y, test_x, test_y = scaler(train_x, train_y, test_x, test_y)\n",
    "    \n",
    "    model = torch.nn.Sequential(\n",
    "    nn.Linear(1000, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1)\n",
    "    )\n",
    "\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        y_pred = model(train_x)\n",
    "        loss = criterion(y_pred, train_y.float())\n",
    "        #print(\"epoch: \", epoch, \"loss: \", loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    pred_y = model(test_x.float())\n",
    "    mae = sum(abs(pred_y - test_y))/len(pred_y)\n",
    "    mae = str(mae.detach().numpy()[0])\n",
    "    return mae\n",
    "    \n",
    "\n",
    "def data_split(data, iteration_split, number_splits, check_for_last_iteration):\n",
    "    \"\"\"Splits data into train and test data.\n",
    "\n",
    "    Args:\n",
    "        data (df): df with all the data\n",
    "        iteration_split (int): current batch step of cross validation\n",
    "        number_splits ([type]): number of batches\n",
    "        check_for_last_iteration (boolean): check if it is the last batch\n",
    "\n",
    "    Returns:\n",
    "        df: train and test data\n",
    "    \"\"\"\n",
    "    if check_for_last_iteration == True:\n",
    "        test = data.iloc[iteration_split * number_splits:]\n",
    "        train = data.drop(data.iloc[iteration_split * number_splits:].index)\n",
    "    else:\n",
    "        test = data.iloc[iteration_split * number_splits : (iteration_split * number_splits + number_splits)]\n",
    "        train = data.drop(data.iloc[iteration_split * number_splits : (iteration_split * number_splits + number_splits)].index)    \n",
    "    \n",
    "    test_x = test.drop(\"target\", axis=1)\n",
    "\n",
    "    test_y = test[\"target\"]\n",
    "\n",
    "    train_x = train.drop(\"target\", axis=1)\n",
    "\n",
    "    train_y = train[\"target\"]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "def cross_validation(model, data, number_splits):\n",
    "    list_mae = []\n",
    "    len_data = len(data)\n",
    "    splits = len_data / number_splits\n",
    "    split = int(splits)\n",
    "    for i in range(number_splits):\n",
    "        check_for_last_iteration = False\n",
    "        if i == number_splits - 1:\n",
    "            check_for_last_iteration = True\n",
    "        train_x, train_y, test_x, test_y = data_split(data, i, split, check_for_last_iteration)\n",
    "        model_result = model_standard(train_x, train_y, test_x, test_y)\n",
    "        list_mae.append(model_result)\n",
    "    print(list_mae)\n",
    "\n",
    "cross_validation_result = cross_validation(model, df_value_target,5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backtesting is if I take 800 instead of 1000 x features for example and than i try to predict the next 200 values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netron architecture visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize model structure with netron\n",
    "model.eval()\n",
    "input_names = ['Values']\n",
    "output_names = ['Targets']\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  train_x, \n",
    "                  \"../model_structure/simple_nn.onnx\", \n",
    "                  input_names=input_names, \n",
    "                  output_names=output_names,\n",
    "                  )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H2OAutoML\n",
    "#Auto-Pytorch from Uni Freiburg\n",
    "#Uber Ludwig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i again start with loading the fucking data would be great to have a method which I can call all the time \n",
    "import h2o\n",
    "h2o.init()\n",
    "h2o.demo(\"glm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uninstall glucose venv -\n",
    "#create glucose venv - \n",
    "#install pyton 3.11 with brew and try to get h2o running \n",
    "#create venv in brew python - \n",
    "#check packages --> needs to be empty - \n",
    "#run first cell of this notebook - \n",
    "#install required python - \n",
    "#install numpy and import numpy in a cell and check if i dont get an error - \n",
    "#install h20 \n",
    "#run a demo succesfully\n",
    "\n",
    "#create virtual env in commandline\n",
    "#activate virtual env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/usr/bin/java', '-version']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/h2o/h2o.py:270\u001b[0m, in \u001b[0;36minit\u001b[0;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     h2oconn \u001b[39m=\u001b[39m H2OConnection\u001b[39m.\u001b[39;49mopen(url\u001b[39m=\u001b[39;49murl, ip\u001b[39m=\u001b[39;49mip, port\u001b[39m=\u001b[39;49mport, name\u001b[39m=\u001b[39;49mname, https\u001b[39m=\u001b[39;49mhttps,\n\u001b[1;32m    271\u001b[0m                                  verify_ssl_certificates\u001b[39m=\u001b[39;49mverify_ssl_certificates, cacert\u001b[39m=\u001b[39;49mcacert,\n\u001b[1;32m    272\u001b[0m                                  auth\u001b[39m=\u001b[39;49mauth, proxy\u001b[39m=\u001b[39;49mproxy, cookies\u001b[39m=\u001b[39;49mcookies, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m                                  msgs\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mChecking whether there is an H2O instance running at \u001b[39;49m\u001b[39m{url}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    274\u001b[0m                                        \u001b[39m\"\u001b[39;49m\u001b[39mconnected.\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mnot found.\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    275\u001b[0m                                  strict_version_check\u001b[39m=\u001b[39;49msvc)\n\u001b[1;32m    276\u001b[0m \u001b[39mexcept\u001b[39;00m H2OConnectionError:\n\u001b[1;32m    277\u001b[0m     \u001b[39m# Backward compatibility: in init() port parameter really meant \"baseport\" when starting a local server...\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/h2o/backend/connection.py:406\u001b[0m, in \u001b[0;36mH2OConnection.open\u001b[0;34m(server, url, ip, port, name, https, auth, verify_ssl_certificates, cacert, proxy, cookies, verbose, msgs, strict_version_check)\u001b[0m\n\u001b[1;32m    405\u001b[0m conn\u001b[39m.\u001b[39m_timeout \u001b[39m=\u001b[39m \u001b[39m3.0\u001b[39m\n\u001b[0;32m--> 406\u001b[0m conn\u001b[39m.\u001b[39m_cluster \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49m_test_connection(retries, messages\u001b[39m=\u001b[39;49mmsgs)\n\u001b[1;32m    407\u001b[0m \u001b[39m# If a server is unable to respond within 1s, it should be considered a bug. However we disable this\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39m# setting for now, for no good reason other than to ignore all those bugs :(\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/h2o/backend/connection.py:712\u001b[0m, in \u001b[0;36mH2OConnection._test_connection\u001b[0;34m(self, max_retries, messages)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 712\u001b[0m     \u001b[39mraise\u001b[39;00m H2OConnectionError(\u001b[39m\"\u001b[39m\u001b[39mCould not establish link to the H2O cloud \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m after \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m retries\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m                              \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_url, max_retries, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(errors)))\n",
      "\u001b[0;31mH2OConnectionError\u001b[0m: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries\n[57:50.57] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10a2e9ab0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n[57:50.78] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10a2ea170>: Failed to establish a new connection: [Errno 61] Connection refused'))\n[57:50.99] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10a2ea8f0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n[57:51.20] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10a2ea0e0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n[57:51.40] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10a2e9810>: Failed to establish a new connection: [Errno 61] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mh2o\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m h2o\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m      3\u001b[0m h2o\u001b[39m.\u001b[39mdemo(\u001b[39m\"\u001b[39m\u001b[39mglm\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/h2o/h2o.py:287\u001b[0m, in \u001b[0;36minit\u001b[0;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[39mif\u001b[39;00m https:\n\u001b[1;32m    284\u001b[0m         \u001b[39mraise\u001b[39;00m H2OConnectionError(\u001b[39m'\u001b[39m\u001b[39mStarting local server is not available with https enabled. You may start local\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    285\u001b[0m                                  \u001b[39m'\u001b[39m\u001b[39m instance of H2O with https manually \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    286\u001b[0m                                  \u001b[39m'\u001b[39m\u001b[39m(https://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#new-user-quick-start).\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 287\u001b[0m     hs \u001b[39m=\u001b[39m H2OLocalServer\u001b[39m.\u001b[39;49mstart(nthreads\u001b[39m=\u001b[39;49mnthreads, enable_assertions\u001b[39m=\u001b[39;49menable_assertions, max_mem_size\u001b[39m=\u001b[39;49mmmax,\n\u001b[1;32m    288\u001b[0m                               min_mem_size\u001b[39m=\u001b[39;49mmmin, ice_root\u001b[39m=\u001b[39;49mice_root, log_dir\u001b[39m=\u001b[39;49mlog_dir, log_level\u001b[39m=\u001b[39;49mlog_level,\n\u001b[1;32m    289\u001b[0m                               max_log_file_size\u001b[39m=\u001b[39;49mmax_log_file_size, port\u001b[39m=\u001b[39;49mport, name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    290\u001b[0m                               extra_classpath\u001b[39m=\u001b[39;49mextra_classpath, jvm_custom_args\u001b[39m=\u001b[39;49mjvm_custom_args,\n\u001b[1;32m    291\u001b[0m                               bind_to_localhost\u001b[39m=\u001b[39;49mbind_to_localhost)\n\u001b[1;32m    292\u001b[0m     h2oconn \u001b[39m=\u001b[39m H2OConnection\u001b[39m.\u001b[39mopen(server\u001b[39m=\u001b[39mhs, https\u001b[39m=\u001b[39mhttps, verify_ssl_certificates\u001b[39m=\u001b[39mverify_ssl_certificates,\n\u001b[1;32m    293\u001b[0m                                  cacert\u001b[39m=\u001b[39mcacert, auth\u001b[39m=\u001b[39mauth, proxy\u001b[39m=\u001b[39mproxy, cookies\u001b[39m=\u001b[39mcookies, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    294\u001b[0m                                  strict_version_check\u001b[39m=\u001b[39msvc)\n\u001b[1;32m    295\u001b[0m h2oconn\u001b[39m.\u001b[39mcluster\u001b[39m.\u001b[39mtimezone \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUTC\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/h2o/backend/server.py:140\u001b[0m, in \u001b[0;36mH2OLocalServer.start\u001b[0;34m(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, log_dir, log_level, max_log_file_size, port, name, extra_classpath, verbose, jvm_custom_args, bind_to_localhost)\u001b[0m\n\u001b[1;32m    137\u001b[0m     hs\u001b[39m.\u001b[39m_tempdir \u001b[39m=\u001b[39m hs\u001b[39m.\u001b[39m_ice_root\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAttempting to start a local H2O server...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m hs\u001b[39m.\u001b[39;49m_launch_server(port\u001b[39m=\u001b[39;49mport, baseport\u001b[39m=\u001b[39;49mbaseport, nthreads\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(nthreads), ea\u001b[39m=\u001b[39;49menable_assertions,\n\u001b[1;32m    141\u001b[0m                   mmax\u001b[39m=\u001b[39;49mmax_mem_size, mmin\u001b[39m=\u001b[39;49mmin_mem_size, jvm_custom_args\u001b[39m=\u001b[39;49mjvm_custom_args,\n\u001b[1;32m    142\u001b[0m                   bind_to_localhost\u001b[39m=\u001b[39;49mbind_to_localhost, log_dir\u001b[39m=\u001b[39;49mlog_dir, log_level\u001b[39m=\u001b[39;49mlog_level, max_log_file_size\u001b[39m=\u001b[39;49mmax_log_file_size)\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  Server is running at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m://\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (hs\u001b[39m.\u001b[39mscheme, hs\u001b[39m.\u001b[39mip, hs\u001b[39m.\u001b[39mport))\n\u001b[1;32m    144\u001b[0m atexit\u001b[39m.\u001b[39mregister(\u001b[39mlambda\u001b[39;00m: hs\u001b[39m.\u001b[39mshutdown())\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/h2o/backend/server.py:273\u001b[0m, in \u001b[0;36mH2OLocalServer._launch_server\u001b[0;34m(self, port, baseport, mmax, mmin, ea, nthreads, jvm_custom_args, bind_to_localhost, log_dir, log_level, max_log_file_size)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m# Find Java and check version. (Note that subprocess.check_output returns the output as a bytes object)\u001b[39;00m\n\u001b[1;32m    272\u001b[0m java \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_find_java()\n\u001b[0;32m--> 273\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_java(java, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_verbose)\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n\u001b[1;32m    276\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  Starting server from \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jar_path)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/h2o/backend/server.py:378\u001b[0m, in \u001b[0;36mH2OLocalServer._check_java\u001b[0;34m(java, verbose)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_java\u001b[39m(java, verbose):\n\u001b[0;32m--> 378\u001b[0m     jver_bytes \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mcheck_output([java, \u001b[39m\"\u001b[39;49m\u001b[39m-version\u001b[39;49m\u001b[39m\"\u001b[39;49m], stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mSTDOUT)\n\u001b[1;32m    379\u001b[0m     jver \u001b[39m=\u001b[39m jver_bytes\u001b[39m.\u001b[39mdecode(encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:421\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m         empty \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    419\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m empty\n\u001b[0;32m--> 421\u001b[0m \u001b[39mreturn\u001b[39;00m run(\u001b[39m*\u001b[39;49mpopenargs, stdout\u001b[39m=\u001b[39;49mPIPE, timeout\u001b[39m=\u001b[39;49mtimeout, check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    422\u001b[0m            \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mstdout\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.8/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m     retcode \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mpoll()\n\u001b[1;32m    525\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[0;32m--> 526\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[1;32m    527\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/usr/bin/java', '-version']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()\n",
    "h2o.demo(\"glm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
